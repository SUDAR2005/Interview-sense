{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40cc1ae5",
   "metadata": {},
   "source": [
    "# Import Langchain library to load and split document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4794866",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Projects\\Resume-ChatBot\\backend\\env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "import os\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from chromadb.config import Settings\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f3ff87",
   "metadata": {},
   "source": [
    "# Load the Document "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0c1b22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_document(path: str):\n",
    "    '''Load a document from a given path'''\n",
    "    # Split the pad and get the extension\n",
    "    _, ext = os.path.splitext(path)\n",
    "    # checks it is pdf\n",
    "    if ext != '.pdf':\n",
    "        raise TypeError(f\"Expected a pyf object received {ext}\")\n",
    "    loader = PyPDFLoader(path)\n",
    "    # return the loader instance\n",
    "    return loader.load()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "690cf769",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(documents):\n",
    "    '''Split the document using text splitter'''\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300, # Size of each chunk in characters\n",
    "    chunk_overlap=100, # Overlap between consecutive chunks\n",
    "    length_function=len, # Function to compute the length of the text\n",
    "    add_start_index=True, # Flag to add start index to each chunk\n",
    "    )\n",
    "    # chunks of splitted document.\n",
    "    texts = splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(texts)} chunks.\")\n",
    "    \n",
    "    print(\"Content: \", texts[0].page_content)\n",
    "    print('Meta data: ', texts[0].metadata)\n",
    "    return texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df64ad3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_vectordb(embedding, texts, directory = './chroma_db'):\n",
    "    # Create a new entry in the database\n",
    "    db = Chroma.from_documents(texts, embedding, persist_directory=directory\n",
    "                               ,client_settings=Settings(anonymized_telemetry=False))\n",
    "    db.persist()\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f89bc0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_store(path, model = 'Gemini'):\n",
    "    # Load the PDF document\n",
    "    documents = load_document(path=path)\n",
    "    # split the text using Recursive Text Splitter\n",
    "    chunks = split_text(documents=documents)\n",
    "    # Create a vector database\n",
    "    if model == 'Gemini':\n",
    "        embedding = GoogleGenerativeAIEmbeddings(model='models/embedding-001', google_api_key=os.getenv('GOOGLE_API_KEY'))\n",
    "        db = store_vectordb(embedding=embedding, texts=chunks, directory='./chroma_db')\n",
    "        return db\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78b437bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 4 documents into 32 chunks.\n",
      "Content:  THIAGARAJAR COLLEGE OF ENGINEERING \n",
      "(An Autonomous Govt-Aided Institution Affiliated to Anna University)  \n",
      "Thiagarajar College Of Engineering, MDU-15 \n",
      " \n",
      " \n",
      "NAME SUDAR MANIKANDAN S \n",
      "DEGREE  B-TECH IT \n",
      "DOB  11.06.2005 \n",
      "EMAIL sudarsettaiyan@gmail.com \n",
      "GITHUB https://github.com/SUDAR2005\n",
      "Meta data:  {'source': './data/Sudar Manikandan S RESUME.pdf', 'page': 0, 'start_index': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n",
      "E:\\temp\\ipykernel_30900\\1471614207.py:5: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  db.persist()\n"
     ]
    }
   ],
   "source": [
    "db = generate_data_store(path='./data/Sudar Manikandan S RESUME.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31232735",
   "metadata": {},
   "outputs": [],
   "source": [
    "hr_prompt_template = \"\"\"\n",
    "You are an experienced HR interviewer conducting a professional interview. Based on the candidate's resume information provided below, ask relevant and insightful questions.\n",
    "\n",
    "RESUME CONTEXT:\n",
    "{context}\n",
    "\n",
    "CONVERSATION HISTORY:\n",
    "{chat_history}\n",
    "\n",
    "CURRENT QUESTION/RESPONSE: {question}\n",
    "\n",
    "Guidelines for your behavior:\n",
    "1. Act as a professional, friendly HR interviewer\n",
    "2. Ask follow-up questions based on the resume content\n",
    "3. Explore technical skills, experience, and soft skills\n",
    "4. Ask about projects, achievements, and career goals\n",
    "5. Keep questions conversational and engaging\n",
    "6. If the candidate asks about the company/role, provide general positive responses\n",
    "7. Gradually progress from basic questions to more detailed technical/behavioral ones\n",
    "\n",
    "If this is the start of conversation, introduce yourself and begin with an opening question.\n",
    "If the candidate has responded, acknowledge their answer and ask appropriate follow-up questions.\n",
    "\n",
    "Response:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77b40720",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"context\", \"chat_history\", \"question\"],\n",
    "    template=hr_prompt_template,\n",
    "    output_variables=[\"Response\"]\n",
    ") \n",
    "llm  = ChatGoogleGenerativeAI(model='gemini-2.5-flash', api_key=os.getenv('GOOGLE_API_KEY'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2fc305",
   "metadata": {},
   "source": [
    "# Build RAG chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "718f4c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnableLambda\n",
    "\n",
    "def build_rag_chain(db: Chroma, prompt, llm):\n",
    "    rag_chain = (\n",
    "    {\"context\": db.as_retriever(), \"chat_history\": RunnablePassthrough(), \"question\": RunnablePassthrough()} \n",
    "    | prompt \n",
    "    | llm\n",
    "    | StrOutputParser() \n",
    "    )\n",
    "    return rag_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1772d29f",
   "metadata": {},
   "source": [
    "# Sample testing part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "619e7ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass the prompt template and llm instance to construct the rag chain\n",
    "rag_chain = build_rag_chain(db, prompt=prompt_template, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "722950dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = prompt_template.invoke({\n",
    "    \"context\": \"Experienced Python developer with AWS and DevOps exposure.\",\n",
    "    \"chat_history\": \"HR: Hi! Can you tell me a bit about your last project?\\nCandidate: Yes, I worked on a scalable microservice architecture using FastAPI...\",\n",
    "    \"question\": \"Can you explain more about the architecture you designed?\"\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5587fbcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are an experienced HR interviewer conducting a professional interview. Based on the candidate's resume information provided below, ask relevant and insightful questions.\n",
      "\n",
      "RESUME CONTEXT:\n",
      "Experienced Python developer with AWS and DevOps exposure.\n",
      "\n",
      "CONVERSATION HISTORY:\n",
      "HR: Hi! Can you tell me a bit about your last project?\n",
      "Candidate: Yes, I worked on a scalable microservice architecture using FastAPI...\n",
      "\n",
      "CURRENT QUESTION/RESPONSE: Can you explain more about the architecture you designed?\n",
      "\n",
      "Guidelines for your behavior:\n",
      "1. Act as a professional, friendly HR interviewer\n",
      "2. Ask follow-up questions based on the resume content\n",
      "3. Explore technical skills, experience, and soft skills\n",
      "4. Ask about projects, achievements, and career goals\n",
      "5. Keep questions conversational and engaging\n",
      "6. If the candidate asks about the company/role, provide general positive responses\n",
      "7. Gradually progress from basic questions to more detailed technical/behavioral ones\n",
      "\n",
      "If this is the start of conversation, introduce yourself and begin with an opening question.\n",
      "If the candidate has responded, acknowledge their answer and ask appropriate follow-up questions.\n",
      "\n",
      "Response:\n"
     ]
    }
   ],
   "source": [
    "print(prompt.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7f2e082",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event CollectionQueryEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Thank you for that explanation. Given that scalability was a key aspect, could you elaborate on which specific AWS services you integrated into the architecture to achieve that, and why you chose them?'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(prompt.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be031967",
   "metadata": {},
   "source": [
    "# Implementing Bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb9853f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = []\n",
    "\n",
    "def format_chat_history(history):\n",
    "    return \"\\n\".join(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04499b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_bot(rag_chain):\n",
    "    print(\"ðŸ‘‹ Welcome to HR Interview Bot. Type 'exit' to quit.\\n\")\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() in ['exit', 'quit']:\n",
    "            print(\"ðŸ‘‹ Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        # format history for prompt\n",
    "        formatted_history = format_chat_history(chat_history)\n",
    "        prompt = prompt_template.invoke({ \n",
    "            \"context\": \"\",\n",
    "            \"chat_history\": formatted_history,\n",
    "            \"question\": user_input\n",
    "        })\n",
    "        response = rag_chain.invoke(prompt.text)\n",
    "        \n",
    "        print(f\"HR: {response}\\n\")\n",
    "        # append to history\n",
    "        chat_history.append(f\"Candidate: {user_input}\")\n",
    "        chat_history.append(f\"HR: {response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426ef4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_with_bot(rag_chain=rag_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a622295a",
   "metadata": {},
   "source": [
    "# Voice Recognition Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6be5529",
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition as sr\n",
    "\n",
    "def get_voice_input():\n",
    "    # recogniser instance\n",
    "    r = sr.Recognizer()\n",
    "    with sr.Microphone() as source:\n",
    "        audio = r.listen(source=source)\n",
    "    try:\n",
    "        text = r.recognize_google(audio) # Using Google Speech Recognition API\n",
    "        print(f\"You said: {text}\")\n",
    "        return text\n",
    "    except sr.UnknownValueError:\n",
    "        print(\"Could not understand audio\")\n",
    "    except sr.RequestError as e:\n",
    "        print(f\"Could not request results; {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b2410bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not understand audio\n"
     ]
    }
   ],
   "source": [
    "text = get_voice_input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e99e467",
   "metadata": {},
   "outputs": [],
   "source": [
    "def speak_with_bot(rag_chain):\n",
    "    print(\"ðŸ‘‹ Welcome to HR Interview Bot. Type 'exit' to quit.\\n\")\n",
    "    while True:\n",
    "        user_input = get_voice_input()\n",
    "        if user_input == \"Could not understand audio\":\n",
    "            print(\"Sorry, I didn't catch that. Please try again.\\n\")\n",
    "            continue\n",
    "        if user_input.lower() in ['exit', 'quit']:\n",
    "            print(\"ðŸ‘‹ Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        # format history for prompt\n",
    "        formatted_history = format_chat_history(chat_history)\n",
    "        prompt = prompt_template.invoke({ \n",
    "            \"context\": \"\",\n",
    "            \"chat_history\": formatted_history,\n",
    "            \"question\": user_input\n",
    "        })\n",
    "        response = rag_chain.invoke(prompt.text)\n",
    "        \n",
    "        print(f\"HR: {response}\\n\")\n",
    "        # append to history\n",
    "        chat_history.append(f\"Candidate: {user_input}\")\n",
    "        chat_history.append(f\"HR: {response}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
